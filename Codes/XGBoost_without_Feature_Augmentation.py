# -*- coding: utf-8 -*-
"""2_Basic XGboost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Htd5WdKKjoIutngf7nc-0DiGpEjQfEMd
"""

#importing required libraries and packages

import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib
import xgboost as xgb
import matplotlib.pyplot as plt
import sklearn
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn import preprocessing
import sklearn.model_selection as ms
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score
from pathlib import Path

#printing out versions of all packages and libraries and used

print(f'pandas version is: {pd.__version__}')
print(f'numpy version is: {np.__version__}')
print(f'matplotlib version is: {matplotlib.__version__}')
print(f'sklearn version is: {sklearn.__version__}')
print(f'xgboost version is: {xgb.__version__}')

import xgboost
from xgboost import XGBClassifier

print("Version:", xgboost.__version__)
print("XGBClassifier from:", XGBClassifier.__module__)
print("xgboost module path:", xgboost.__file__)

#all helper functions used

def drop_columns(data, *args):

    '''
    function used to drop columns.
    args::
      data:  dataframe to be operated on
      *args: a list of columns to be dropped from the dataframe

    return: returns a dataframe with the columns dropped
    '''

    columns = []
    for _ in args:
        columns.append(_)

    data = data.drop(columns, axis=1)

    return data

def process(data):

    '''
    function to process dataframe by replacing missing, infinity values with -999

    args::
      data:  dataframe to be operated on

    returns dataframe with replaced values
    '''

    cols = list(data.columns)
    for _ in cols:

        data[_] = np.where(data[_] == np.inf, -999, data[_])
        data[_] = np.where(data[_] == np.nan, -999, data[_])
        data[_] = np.where(data[_] == -np.inf, -999, data[_])

    return data

def show_evaluation(pred, true):

  '''

  function to show model performance and evaluation
  args:
    pred: predicted value(a list)
    true: actual values (a list)

  prints the custom metric performance, accuracy and F1 score of predictions

  '''

  print(f'Default score: {score(true.values, pred)}')
  print(f'Accuracy is: {accuracy_score(true, pred)}')
  print(f'F1 is: {f1_score(pred, true.values, average="weighted")}')


def score(y_true, y_pred):

    '''
    custom metric used for evaluation
    args:
      y_true: actual prediction
      y_pred: predictions made
    '''

    S = 0.0
    y_true = y_true.astype(int)
    y_pred = y_pred.astype(int)
    for i in range(0, y_true.shape[0]):
        S -= A[y_true[i], y_pred[i]]
    return S/y_true.shape[0]

#from google.colab import drive
#drive.mount('/content/drive')

#should be edited to the present working directory of the user
#PWD = '/content/drive/My Drive/FORCE-Lithology-Prediction/'

# Define base directory relative to your known home
base_dir = Path("/Users/Engineering User/Desktop/Force_Lith_Classification/Data")

#importing penaltry matrix used for evaluation and train and test files
A = np.load(base_dir / 'penalty_matrix.npy')

train = pd.read_csv(base_dir / 'train.csv', sep=';')

test = pd.read_csv(base_dir / 'hidden_test.csv', sep=';')

class Model():

    '''
    class to lithology prediction
    '''

    def __init__(self, train, test):

        '''
        takes in the train and test dataframes
        '''

        self.train = train
        self.test = test


    def __call__(self, plot = True):

      return self.fit(plot)

    def preprocess(self, train, test):

        '''
        method to prepare datasets for training and predictions
        accepts both the train and test dataframes as arguments

        returns the prepared train, test datasets along with the
        lithology labels and numbers which is needed for preparing
        the submission file

        '''

        #concatenating both train and test datasets for easier and uniform processing

        ntrain = train.shape[0]
        ntest = test.shape[0]
        target = train.FORCE_2020_LITHOFACIES_LITHOLOGY.copy()
        df = pd.concat((train, test)).reset_index(drop=True)

        #mapping the lithology labels to ordinal values for better modelling

        lithology = train['FORCE_2020_LITHOFACIES_LITHOLOGY']

        lithology_numbers = {30000: 0,
                        65030: 1,
                        65000: 2,
                        80000: 3,
                        74000: 4,
                        70000: 5,
                        70032: 6,
                        88000: 7,
                        86000: 8,
                        99000: 9,
                        90000: 10,
                        93000: 11}

        lithology1 = lithology.map(lithology_numbers)

        #implementing Bestagini's augmentation procedure

        train_well = train.WELL.values
        train_depth = train.DEPTH_MD.values

        test_well = test.WELL.values
        test_depth = test.DEPTH_MD.values
        '''to be continued...
        #this was done here for ease as the datasets would undergo some transformations
        #that would make it uneasy to perform the augmentation technique'''



        print(f'shape of concatenated dataframe before dropping columns {df.shape}')

        cols = ['FORCE_2020_LITHOFACIES_CONFIDENCE', 'SGR', 'DTS', 'RXO', 'ROPA'] #columns to be dropped
        df = drop_columns(df, *cols)
        print(f'shape of dataframe after dropping columns {df.shape}')
        print(f'{cols} were dropped')

        #Label encoding the GROUP, FORMATION and WELLS features as these improved the performance of the models on validations

        df['GROUP_encoded'] = df['GROUP'].astype('category')
        df['GROUP_encoded'] = df['GROUP_encoded'].cat.codes
        df['FORMATION_encoded'] = df['FORMATION'].astype('category')
        df['FORMATION_encoded'] = df['FORMATION_encoded'].cat.codes
        df['WELL_encoded'] = df['WELL'].astype('category')
        df['WELL_encoded'] = df['WELL_encoded'].cat.codes
        print(f'shape of dataframe after label encoding columns {df.shape}')


        #FURTHER PREPRATION TO SPLIT DATAFRAME INTO TRAIN AND TEST DATASETS AFTER PREPRATION
        print(f'Splitting concatenated dataframe into training and test datasets...')
        df = df.drop(['WELL', 'GROUP', 'FORMATION'], axis=1)
        print(df.shape)

        df = df.fillna(-999)
        df = process(df)
        data = df.copy()

        train2 = data[:ntrain].copy()
        train2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)

        test2 = data[ntrain:(ntest+ntrain)].copy()
        test2.drop(['FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1, inplace=True)
        test2 = test2.reset_index(drop=True)

        traindata = train2
        testdata = test2

        print(f'Shape of train and test datasets before augmentation {traindata.shape, testdata.shape}')

        #traindata1, padded_rows = augment_features(pd.DataFrame(traindata).values, train_well, train_depth)
        #testdata1, padded_rows = augment_features(pd.DataFrame(testdata).values, test_well, test_depth)
        traindata1 = pd.DataFrame(traindata).values
        testdata1 = pd.DataFrame(testdata).values

        print(f'Shape of train and test datasets after augmentation {traindata1.shape, testdata1.shape}')



        return traindata1, testdata1, lithology1, lithology_numbers


    def fit(self, plot):

      '''
      method to train model and make predictions

      returns the test predictions, trained model, and lithology numbers used for making the submission file
      '''

      traindata1, testdata1, lithology1, lithology_numbers = self.preprocess(self.train, self.test)

      #using a 10-fold stratified cross-validation technique and seting the shuffle parameter to true
      #as this improved the validation performance better

      split = 10
      kf = StratifiedKFold(n_splits=split, shuffle=True)

      open_test = np.zeros((len(testdata1), 12))

      #100 n-estimators and 10 max-depth
      model = XGBClassifier(n_estimators=100, max_depth=10, booster='gbtree',
                            objective='multi:softprob', learning_rate=0.1, random_state=0,
                            subsample=0.9, colsample_bytree=0.9,
                            eval_metric='mlogloss', verbose=2020, reg_lambda=1500, early_stopping_rounds=100)


      i = 1
      for (train_index, test_index) in kf.split(pd.DataFrame(traindata1), pd.DataFrame(lithology1)):
        X_train, X_test = pd.DataFrame(traindata1).iloc[train_index], pd.DataFrame(traindata1).iloc[test_index]
        Y_train, Y_test = pd.DataFrame(lithology1).iloc[train_index],pd.DataFrame(lithology1).iloc[test_index]

        model.fit(X_train, Y_train, eval_set=[(X_test, Y_test)], verbose=100)
        prediction = model.predict(X_test)
        print(show_evaluation(prediction, Y_test))

        print(f'-----------------------FOLD {i}---------------------')
        i+=1

        open_test += model.predict_proba(pd.DataFrame(testdata1))

      open_test= pd.DataFrame(open_test/split)

      open_test = np.array(pd.DataFrame(open_test).idxmax(axis=1))

      print('---------------CROSS VALIDATION COMPLETE')
      print('----------------TEST EVALUATION------------------')


      if plot: self.plot_feat_imp(model)
      return open_test, model, lithology_numbers


    def plot_feat_imp(self, model):
        feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)
        plt.figure(figsize=(12,8))
        feat_imp.plot(kind='bar', title='Feature Importances')
        plt.ylabel('Feature Importance Score')

    def make_submission_file(self, filename):

      '''
      method to train model, make prediction and create submission file
      args::
        filename: name to save submission file as (string)
      '''

      self.filename = filename

      prediction, model, lithology_numbers = self.fit(plot=False)

      #path = PWD

      test = pd.read_csv(base_dir /'hidden_test.csv', sep=';')

      category_to_lithology = {y:x for x,y in lithology_numbers.items()}
      test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)
      np.savetxt(path+filename+'.csv', test_prediction_for_submission, header='lithology', fmt='%i')

#To train model and make prediction

func_= Model(train, test)
prediction, model, redundant = func_()

prediction

len(prediction)

from pathlib import Path
import numpy as np

lithology_numbers = {30000: 0,
                65030: 1,
                65000: 2,
                80000: 3,
                74000: 4,
                70000: 5,
                70032: 6,
                88000: 7,
                86000: 8,
                99000: 9,
                90000: 10,
                93000: 11}
# Set base directory (adjust if needed)
base_dir_N = Path("/Users/Engineering User/Desktop/Force_Lith_Classification/Notebook")

# Map predicted class numbers back to original lithology codes
category_to_lithology = {y: x for x, y in lithology_numbers.items()}
test_prediction_for_submission = np.vectorize(category_to_lithology.get)(prediction)

# Define full path to save file
filename = "Olawale_Predictions_BaseXGB"
output_file = base_dir_N / f"{filename}.csv"

# Save the prediction file
np.savetxt(output_file, test_prediction_for_submission, header='lithology', fmt='%i', comments='')

len(test_prediction_for_submission)

type(test_prediction_for_submission)

"""# Just Run Separate: Comparing with actual test predictions"""

# Define base directory relative to your known home
base_dir = Path("/Users/Engineering User/Desktop/Force_Lith_Classification/Data")

test = pd.read_csv(base_dir / 'hidden_test.csv', sep=';')

test.columns

actual_class = test['FORCE_2020_LITHOFACIES_LITHOLOGY'].to_numpy()

# Define base directory relative to your known home
base_dir_predict = Path("/Users/Engineering User/Desktop/Force_Lith_Classification/Notebook")

pred_df = pd.read_csv(base_dir_predict / 'Olawale_Predictions_BaseXGB.csv', sep=';')
pred_df

test_prediction_for_submission = pred_df['lithology'].to_numpy()

test_prediction_for_submission

from sklearn.metrics import accuracy_score

acc = accuracy_score(actual_class, test_prediction_for_submission)
print(f"Accuracy: {acc:.4f}")

from sklearn.metrics import f1_score

f1 = f1_score(actual_class, test_prediction_for_submission, average='weighted')
print(f"Weighted F1 Score: {f1:.4f}")

# Example: test_prediction_for_submission = np.array([...])
unique_values, counts = np.unique(test_prediction_for_submission, return_counts=True)

# Display results
for val, count in zip(unique_values, counts):
    print(f"Value {val}: {count} times")

# Example: test_prediction_for_submission = np.array([...])
unique_values, counts = np.unique(actual_class, return_counts=True)

# Display results
for val, count in zip(unique_values, counts):
    print(f"Value {val}: {count} times")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Original label mapping (in desired order)
lithology_keys = {
    30000: 'Sandstone',
    65030: 'Sandstone/Shale',
    65000: 'Shale',
    80000: 'Marl',
    74000: 'Dolomite',
    70000: 'Limestone',
    70032: 'Chalk',
    88000: 'Halite',
    86000: 'Anhydrite',
    99000: 'Tuff',
    90000: 'Coals',
    93000: 'Basement'
}

# Convert actual and predicted to string labels using the mapping
actual_labels = np.vectorize(lithology_keys.get)(actual_class)
predicted_labels = np.vectorize(lithology_keys.get)(test_prediction_for_submission)

# Example: test_prediction_for_submission = np.array([...])
unique_values, counts = np.unique(predicted_labels, return_counts=True)

# Display results
for val, count in zip(unique_values, counts):
    print(f"Value {val}: {count} times")

# Example: test_prediction_for_submission = np.array([...])
unique_values, counts = np.unique(actual_labels, return_counts=True)

# Display results
for val, count in zip(unique_values, counts):
    print(f"Value {val}: {count} times")

from sklearn.metrics import confusion_matrix

# Example arrays
actual = actual_class
predicted = test_prediction_for_submission

# Construct confusion matrix
cm = confusion_matrix(actual, predicted)

print("Confusion Matrix:")
print(cm)

import numpy as np

unique, counts = np.unique(actual_class, return_counts=True)
class_distribution = dict(zip(unique, counts))

for lith, count in class_distribution.items():
    print(f"Lithology {lith}: {count} samples")

import numpy as np

unique, counts = np.unique(test_prediction_for_submission, return_counts=True)
class_distribution = dict(zip(unique, counts))

for lith, count in class_distribution.items():
    print(f"Lithology {lith}: {count} samples")

# Ensure actual and predicted labels are string-mapped
actual_labels = np.vectorize(lithology_keys.get)(actual_class)
predicted_labels = np.vectorize(lithology_keys.get)(test_prediction_for_submission)

# Keep label order
ordered_labels = list(lithology_keys.values())

# Generate confusion matrix
cm = confusion_matrix(actual_labels, predicted_labels, labels=ordered_labels)

# Convert to DataFrame for clean display
df_cm = pd.DataFrame(cm, index=ordered_labels, columns=ordered_labels)

# Display
print("Confusion Matrix (Raw Counts):")
df_cm

import matplotlib.pyplot as plt
import numpy as np

# Data to plot
data = df_cm.values
row_labels = df_cm.index.tolist()
col_labels = df_cm.columns.tolist()

fig, ax = plt.subplots(figsize=(14, 10))
cmap = plt.cm.YlGn  # Use the same colormap
im = ax.imshow(data, cmap=cmap)

# Add colorbar (gradient legend)
cbar = ax.figure.colorbar(im, ax=ax)
cbar.ax.set_ylabel("Count", rotation=90, va="bottom", fontsize=14)
cbar.ax.tick_params(labelsize=12)

# Show ticks and labels
ax.set_xticks(np.arange(len(col_labels)))
ax.set_yticks(np.arange(len(row_labels)))
ax.set_xticklabels(col_labels, rotation=45, ha='right', fontsize=12)
ax.set_yticklabels(row_labels, fontsize=12)

# Gridlines
ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)
ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)
ax.grid(which="minor", color='gray', linestyle='-', linewidth=0.5)
ax.tick_params(which="minor", bottom=False, left=False)

# Annotate cells with counts
for i in range(data.shape[0]):
    for j in range(data.shape[1]):
        ax.text(j, i, data[i, j], ha="center", va="center", color="black", fontsize=10)

# Titles and labels
#ax.set_title("Confusion Matrix of XGBoost without Data Augmentation", fontsize=16, pad=20)
ax.set_xlabel("Predicted Class", fontsize=14)
ax.set_ylabel("True Class", fontsize=14)
plt.tight_layout()
plt.savefig("confusion_matrix_xgboost_without_augmentation.pdf", format="pdf", bbox_inches="tight")

plt.show()

from sklearn.metrics import classification_report
import numpy as np

# Map numeric predictions/actuals to strings
actual_labels = np.vectorize(lithology_keys.get)(actual_class)
predicted_labels = np.vectorize(lithology_keys.get)(test_prediction_for_submission)

# Ensure consistent label order
ordered_labels = list(lithology_keys.values())

# Generate report
report = classification_report(actual_labels, predicted_labels, labels=ordered_labels, zero_division=0)

# Print
print("Classification Report:\n")
print(report)

from sklearn.metrics import accuracy_score

# Assuming you have your true and predicted values
accuracy = accuracy_score(actual_labels, predicted_labels)

print(f"Accuracy: {accuracy:.4f}")

def score(y_true, y_pred):

    '''
    custom metric used for evaluation
    args:
      y_true: actual prediction
      y_pred: predictions made
    '''

    S = 0.0
    y_true = y_true.astype(int)
    y_pred = y_pred.astype(int)
    for i in range(0, y_true.shape[0]):
        S -= A[y_true[i], y_pred[i]]
    return S/y_true.shape[0]

"""# Final Scoring FORCE METRIC"""

A

def score(y_true, y_pred):

    '''
    custom metric used for evaluation
    args:
      y_true: actual prediction
      y_pred: predictions made
    '''

    S = 0.0
    y_true = y_true.astype(int)
    y_pred = y_pred.astype(int)
    for i in range(0, y_true.shape[0]):
        S -= A[y_true[i], y_pred[i]]
    return S/y_true.shape[0]

prediction

actual

lithology_numbers = {
    30000: 0,
    65030: 1,
    65000: 2,
    80000: 3,
    74000: 4,
    70000: 5,
    70032: 6,
    88000: 7,
    86000: 8,
    99000: 9,
    90000: 10,
    93000: 11
}
# Map using vectorized approach
y_true = np.vectorize(lithology_numbers.get)(actual)
y_true

custom_score = score(y_true, prediction)
custom_score